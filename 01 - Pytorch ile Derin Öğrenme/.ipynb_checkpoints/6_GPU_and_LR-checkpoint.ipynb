{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "879456e5",
   "metadata": {},
   "source": [
    "### Kütüphaneler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a21341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim  \n",
    "import torchvision.transforms as transforms \n",
    "import torchvision\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import (Dataset,DataLoader) \n",
    "from skimage import io\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2712ff7",
   "metadata": {},
   "source": [
    "### Veriyi Dahil Etme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abacc03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class veri(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = io.imread(img_path)\n",
    "        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return (image, y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e95d09",
   "metadata": {},
   "source": [
    "### Veriyi Hazırlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8056ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = veri(csv_file = r\"f113.csv\",\n",
    "              root_dir = r\"f1_classification\",\n",
    "              transform = transforms.Compose([\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Resize(size = (28, 28)),\n",
    "                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423e2c05",
   "metadata": {},
   "source": [
    "### Veri Ön İşleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a58d04ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,  test_set = torch.utils.data.random_split(dataset, [200,79]) \n",
    "train_loader = DataLoader(dataset=train_set, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd23ed6",
   "metadata": {},
   "source": [
    "### Model Mimarisini Oluşturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ffb2658",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=4, kernel_size=(5,5))\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=(3,3))\n",
    "        self.conv3 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(2,2)) #22\n",
    "        self.conv4 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(2,2)) #10\n",
    "        \n",
    "        self.max = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        \n",
    "        self.func = nn.ELU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=32, out_features=50)\n",
    "        #self.fc1 = nn.Linear(in_features=519840, out_features=50)\n",
    "        #self.fc1 = nn.Linear(in_features=32*100*100, out_features=50)\n",
    "        self.fc2 = nn.Linear(in_features=50, out_features=50)\n",
    "        self.fc3 = nn.Linear(in_features=50, out_features=100)\n",
    "        self.fc4 = nn.Linear(in_features=100, out_features=4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.func(x)\n",
    "        \n",
    "        x = self.max(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.func(x)\n",
    "        \n",
    "        x = self.max(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.func(x)\n",
    "        \n",
    "        x = self.max(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.func(x)\n",
    "        \n",
    "        #flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #x = x.view(x.size(0), 519840)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.func(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.func(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.func(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac9f3db",
   "metadata": {},
   "source": [
    "### Modelin Eğitimi ve Testi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af66983c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100 Loss: 1.4336787462234497 Accuracy: 24.05063247680664% Error: 75.9493637084961%\n",
      "Iteration: 200 Loss: 1.0935814380645752 Accuracy: 40.50632858276367% Error: 59.49367141723633%\n",
      "Iteration: 300 Loss: 0.34756943583488464 Accuracy: 60.75949478149414% Error: 39.24050521850586%\n",
      "Iteration: 400 Loss: 1.228851318359375 Accuracy: 63.2911376953125% Error: 36.7088623046875%\n",
      "Iteration: 500 Loss: 0.17719678580760956 Accuracy: 67.08860778808594% Error: 32.91139221191406%\n",
      "Iteration: 600 Loss: 1.1642106771469116 Accuracy: 65.82278442382812% Error: 34.177215576171875%\n",
      "Iteration: 700 Loss: 0.21224819123744965 Accuracy: 72.15190124511719% Error: 27.848100662231445%\n",
      "Iteration: 800 Loss: 0.9057134985923767 Accuracy: 75.9493637084961% Error: 24.05063247680664%\n",
      "Iteration: 900 Loss: 0.19107042253017426 Accuracy: 72.15190124511719% Error: 27.848100662231445%\n",
      "Iteration: 1000 Loss: 1.0221753120422363 Accuracy: 77.2151870727539% Error: 22.78481101989746%\n",
      "Iteration: 1100 Loss: 0.03812174126505852 Accuracy: 79.74683380126953% Error: 20.253164291381836%\n",
      "Iteration: 1200 Loss: 0.9714556336402893 Accuracy: 73.417724609375% Error: 26.582279205322266%\n",
      "Iteration: 1300 Loss: 0.2080104947090149 Accuracy: 73.417724609375% Error: 26.582279205322266%\n",
      "Iteration: 1400 Loss: 0.8429573178291321 Accuracy: 78.48101043701172% Error: 21.51898765563965%\n",
      "Iteration: 1500 Loss: 0.001680511049926281 Accuracy: 79.74683380126953% Error: 20.253164291381836%\n",
      "Iteration: 1600 Loss: 0.34692519903182983 Accuracy: 87.3417739868164% Error: 12.658227920532227%\n",
      "Iteration: 1700 Loss: 0.0007700338610447943 Accuracy: 84.81012725830078% Error: 15.189873695373535%\n",
      "Iteration: 1800 Loss: 0.5030338764190674 Accuracy: 79.74683380126953% Error: 20.253164291381836%\n",
      "Iteration: 1900 Loss: 0.0002623452164698392 Accuracy: 86.0759506225586% Error: 13.924050331115723%\n",
      "Iteration: 2000 Loss: 0.5260274410247803 Accuracy: 78.48101043701172% Error: 21.51898765563965%\n",
      "Süre:  215.35468006134033\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model = Net()\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "error = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "epoch = 10\n",
    "count = 0\n",
    "loss_list  = []\n",
    "iteration_list = []\n",
    "acc_list = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    for images, label in train_loader:\n",
    "        \n",
    "        tahmin = model(images)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = error(tahmin, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        count+=1\n",
    "        \n",
    "        if count % 100 == 0:\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            correct_hata = 0\n",
    "            \n",
    "            for images, labels in test_loader:\n",
    "                out = model(images.float())\n",
    "                pred = torch.max(out.data, 1)[1]\n",
    "                total += len(labels)\n",
    "                \n",
    "                correct += (pred == labels).sum()\n",
    "                correct_hata += (pred != labels).sum()\n",
    "                \n",
    "            dogruluk = 100 * correct / float(total)\n",
    "            hata = 100 * correct_hata / float(total)\n",
    "        \n",
    "            loss_list.append(loss.data)\n",
    "            acc_list.append(dogruluk)\n",
    "            iteration_list.append(count)\n",
    "            \n",
    "        if count % 100 == 0:\n",
    "            print(f\"Iteration: {count} Loss: {loss.data} Accuracy: {dogruluk}% Error: {hata}%\")\n",
    "            \n",
    "end = time.time()\n",
    "print(\"Süre: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a46677f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
